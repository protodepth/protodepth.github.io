<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We present ProtoDepth, a novel approach for unsupervised continual depth completion that mitigates catastrophic forgetting by leveraging prototype sets as selective biases.">
  <meta name="keywords" content="ProtoDepth, Proto, Depth, Completion, Estimation, Unsupervised, Prototypes">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/yale-cropped.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://patrickqrim.github.io/">Patrick Rim</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/hyoungseob-park-00692a188/">Hyoungseob Park</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/suchisrit/">S. Gangopadhyay</a>,
            </span>
            <span class="author-block">
              <a href="https://adonis-galaxy.github.io/homepage/">Ziyao Zeng</a>,
            </span>
            <span class="author-block">
              <a href="https://fuzzythecat.github.io/">Younjoon Chung</a>,
            </span>
            <span class="author-block">
              <a href="https://vision.cs.yale.edu/members/alex-wong.html">Alex Wong</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Yale Vision Lab</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CVPR 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.12745"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.12745"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/patrickqrim/ProtoDepth"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <style>
      .demo-img {
        width: 520px;
        height: auto;
      }
      .label-text {
        color: black;
        margin-bottom: 5px; /* reduced space between label and image */
      }
    </style>
    <div class="columns is-vcentered" style="gap: 10px;">
      <div class="column has-text-centered">
        <p class="subtitle label-text">NYUv2 Scene</p>
        <img class="demo-img" src="./static/demos/scene.png" alt="NYUv2 Scene">
      </div>
      <div class="column has-text-centered">
        <p class="subtitle label-text">CMP [CVPR '24]</p>
        <img class="demo-img" src="./static/demos/base_demo.gif" alt="CMP [CVPR '24]">
      </div>
      <div class="column has-text-centered">
        <p class="subtitle label-text">Ours [CVPR '25]</p>
        <img class="demo-img" src="./static/demos/ours_demo.gif" alt="Ours [CVPR '25]">
      </div>
    </div>
    <div class="has-text-centered" style="margin-top: 20px;">
      <p class="subtitle">
        Demonstrating robust continual depth completion: performance in the challenging mixed setting of adapting from indoor (NYUv2) to outdoor (KITTI) environments.
      </p>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present ProtoDepth, a novel prototype-based approach for continual learning of unsupervised depth completion, the multimodal 3D reconstruction task of predicting dense depth maps from RGB images and sparse point clouds. The unsupervised learning paradigm is well-suited for continual learning, as ground truth is not needed. However, when training on new non-stationary distributions, depth completion models will catastrophically forget previously learned information. We address forgetting by learning prototype sets that adapt the latent features of a frozen pretrained model to new domains. Since the original weights are not modified, ProtoDepth does not forget when test-time domain identity is known. To extend ProtoDepth to the challenging setting where the test-time domain identity is withheld, we propose to learn domain descriptors that enable the model to select the appropriate prototype set for inference. We evaluate ProtoDepth on benchmark dataset sequences, where we reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for outdoor to achieve the state of the art.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-centered">
          <img src="./static/images/main_figure.png" alt="Method Overview" style="max-width: 100%; height: auto; margin-bottom: 5px;">
          <p>
            <strong>Overview of ProtoDepth.</strong> (a) In the agnostic setting, a prototype set is selected by maximizing the cosine similarity between an input sample descriptor and the learned domain descriptors. In the incremental setting, the domain identity is known. (b)
            At inference, the similarity between the frozen queries and the keys of the selected prototype set determines how the learned prototypes contribute as local (additive) biases to the latent features. Additionally, a global (multiplicative) bias is applied using a 1Ã—1 depthwise convolution.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-centered">
          <img src="./static/images/qualitative.png" alt="Qualitative Comparison" style="max-width: 100%; height: auto; margin-bottom: 0px;">
          <p>
            <strong>Qualitative comparison</strong> of ProtoDepth and baseline methods using VOICED on <strong>KITTI</strong> after continual training on <strong>Waymo</strong>. (a) Input sample from KITTI, (b) Baseline methods exhibit significant forgetting, particularly for small-surface-area objects (e.g., street signs and lamp posts) where sparse depth is limited, and photometric priors from KITTI are critical. In contrast, ProtoDepth produces high-fidelity depth predictions, effectively mitigating forgetting despite the large domain gap between KITTI and Waymo.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Indoor Performance</h2>
        <div class="content has-text-centered">
          <img src="./static/images/indoor_results.png" alt="Qualitative Comparison" style="max-width: 100%; height: auto; margin-bottom: 5px;">
          <p>
            <strong>Quantitative results</strong> on <strong>indoor</strong> datasets. Models are initially trained on NYUv2 and continually trained on ScanNet, then VOID.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Outdoor Performance</h2>
        <div class="content has-text-centered">
          <img src="./static/images/outdoor_results.png" alt="Qualitative Comparison" style="max-width: 100%; height: auto; margin-bottom: 5px;">
          <p>
            <strong>Quantitative results</strong> on <strong>outdoor</strong> datasets. Models are initially trained on KITTI and continually trained on Waymo, then VKITTI.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Domain Descriptor Analysis</h2>
    <div class="columns is-vcentered">
      <!-- Left: Image -->
      <div class="column is-half has-text-centered">
        <img src="./static/images/indoor_tsne.png" alt="Domain Descriptor t-SNE" style="max-width: 100%; height: auto;">
      </div>
      <!-- Right: Text -->
      <div class="column is-half">
        <p class="subtitle has-text-left">
          <strong>t-SNE plot</strong> of KBNet sample descriptors for indoor validation datasets (NYUv2, ScanNet, VOID) and their respective domain descriptors learned during training in the agnostic setting. While most sample descriptors align closely with their respective domain descriptors, some overlap enables cross-domain generalization, improving performance in challenging scenarios.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{rim2025protodepth,
      title={ProtoDepth: Unsupervised Continual Depth Completion with Prototypes},
      author={Rim, Patrick and Park, Hyoungseob and Gangopadhyay, S and Zeng, Ziyao and Chung, Younjoon and Wong, Alex},
      journal={arXiv preprint arXiv:2503.12745},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template comes from <a
              href="https://github.com/nerfies/nerfies.github.io">this</a> awesome project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
